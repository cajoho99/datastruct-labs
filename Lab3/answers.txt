Running the slow program
------------------------

Q: What is the complexity of findSimilarity?
Answer in terms of N, the total number of 5-grams in the input files.
Assume that the number of 5-grams that occur in more than one file is a small
constant - that is, there is not much plagiarised text.

A: 
The amount of files is D which gets compared to (D-1) other files with an average
number K n-grams in each file. For every n-gram we also need to compare it to every other n-gram. 
This leads to: D*(D-1)*K² = (D² - D)*K² = D²*K² - D*K². Knowing that N = D*K, and disregarding any 
lower terms of complexity we find that the complexity for findSimilarity is N^2.


Q: How long did the program take on the 'small' and 'medium' directories?
Is the ratio between the times what you would expect, given the complexity?
Explain very briefly why.

A: 
Small: 2.8s
Medium: 434s
Given the number of n-grams has changed by a factor of 10. Given our assumtion 
about the complexity we would expect a time increase of 100 which is what we observe.
Assuming that this conclusion is correct it seams that the number of files, which increased a significant
amount, did not have a noticable inpact on the performance. This reaffirmes our belief
that the order of growth is primarily based on the number of n-grams.


Q: How long do you predict the program would take to run on the 'huge'
directory?

A: The number of n-grams has increased by a factor of 20. Therefore the time should
increase by a factor of 400 leading to a time of around 200 000s. 

Using binary search trees
-------------------------

Q: Which of the trees usually become unbalanced?

A: The file tree usually becomes unbalanced. It's height is often size - 1. This implies that the 
tree is only one long chain, which is very unbalanced. We do not know how severly unbalanced 
the other trees are. We seem to find that they are reasonably balanced and therefore do not 
know how to improve them in a simple way.

Q (optional): Is there a simple way to stop these trees becoming unbalanced?

A (optional):  Files seams to be read in a ordered format. If you scrable the order of the files the performance
would improve. 


Using scapegoat trees
---------------------

Q: Now what is the total complexity of buildIndex and findSimilarity?
Again, assume a total of N 5-grams, and a constant number of 5-grams that
occur in more than one file.

A: 
Files: D 
All 5-Grams in one file: K

For buildIndex we analyze every file and with every 5-gram in it. We loop through every 5-gram once and add the
path either to an existing 5-gram in the index or add a new list of path for that 5-gram. This leads to the complexity 
being N which is approximately linear due to the low number of files compared to the 5-grams. But we also need
to keep in mind that we are populating a scapegoat tree where the time to add N elements takes N lg N time. 
As such, total complexity for buildIndex is N lg N, which is linearthmethic. 

Total number of unique 5-grams: P

For the findSimilarity method we loop though every 5-gram found when we call buildIndex. We then check if
there is more than one file that has this 5-gram in it. If such is the case, for every possible pair of these files, 
excuding order ( a,b == b,a ), we create a PathPair. We either add this pair to similarity if it is new or we increment 
this pair if it already exists. Because we loop though every 5-gram the total number of pairs that we create is 
very small compared to the number of 5-grams. Therefore when determening complexity we exclude file 
combinations and determine the complexity to be linear P. Also since we are only using a scapegoat tree for 
file pairs and we are ignoring those for findSimilarity, the usage of scapegoat trees does not inpact the complexity 
in a meaningful way.

Therefore the total complexity for buildIndex and findSimilarity is: N lg N which computes to linearthmethic 
as that is the highest complexity.

Q (optional): What if the total similarity score is an arbitrary number S,
rather than a small constant?

A (optional): If S a large number that means that we need to create more path pairs. This means we path pairs in account 
when calculating complexity for findSimilarity. Worst case all files are equal leading to a pair needing to be created between 
every file. This gives findSimilarity the complexity of N*S². 

Q (optional): Did you find any text that was clearly copied?

A (optional): 
